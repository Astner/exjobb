@InProceedings{Hutter-2001,
  author =       "Marcus Hutter",
  title =        "Distribution of Mutual Information",
  _number =       "IDSIA-13-01",
  booktitle =    "Advances in Neural Information Processing Systems 14",
  editor =       "T. G. Dietterich and S. Becker and Z. Ghahramani",
  publisher =    "MIT Press",
  address =      "Cambridge, MA",
  pages =        "399--406",
  year =         "2002",
  url =          "http://www.hutter1.net/ai/xentropy.htm",
  url2 =         "http://arxiv.org/abs/cs.AI/0112019",
  ftp =          "ftp://ftp.idsia.ch/pub/techrep/IDSIA-13-01.ps.gz",
  categories =   "I.2.   [Artificial Intelligence]",
  keywords =     "Mutual Information, Cross Entropy, Dirichlet distribution, Second
                  order distribution, expectation and variance of mutual
                  information.",
  abstract =     "The mutual information of two random variables i and j with joint
                  probabilities t_ij is commonly used in learning Bayesian nets as
                  well as in many other fields. The chances t_ij are usually
                  estimated by the empirical sampling frequency n_ij/n leading to a
                  point estimate I(n_ij/n) for the mutual information. To answer
                  questions like ``is I(n_ij/n) consistent with zero?'' or ``what is
                  the probability that the true mutual information is much larger
                  than the point estimate?'' one has to go beyond the point estimate.
                  In the Bayesian framework one can answer these questions by
                  utilizing a (second order) prior distribution p(t) comprising
                  prior information about t. From the prior p(t) one can compute the
                  posterior p(t|n), from which the distribution p(I|n) of the mutual
                  information can be calculated. We derive reliable and quickly
                  computable approximations for p(I|n). We concentrate on the mean,
                  variance, skewness, and kurtosis, and non-informative priors. For
                  the mean we also give an exact expression. Numerical issues and
                  the range of validity are discussed.",
}

@TechReport{Wolf-1993,
  author = {%D. R. Wolf and D. H. Wolpert},
  title = {Estimating functions of distributions from A finite set of samples, part 2: Bayes estimators for mutual information, chi- squared, covariance and other statistics},
  institution = {Los Alamos National Laboratory},
  year = {1993},
  number = {LANL-LA-UR-93- 833}
}

@Book{Cover-1991,
  author = {T. M. Cover and J. A. Thomas},
  title = {Elements of Information Theory},
  publisher = {John Wiley \& Sons},
  year = {1991},
  series = {Wiley Series in Telecommunications},
  address = {New York, NY, USA}
}

@Article{Church-1990,
  author = {Kenneth Ward Church and Patrick Hanks},
  title = {Word association norms, mutual information, and lexicography},
  journal = {Computational Linguistics},
  year = {1990},
  volume = {16},
  number = {1},
  pages = {22--29},
  month = {March}
}
