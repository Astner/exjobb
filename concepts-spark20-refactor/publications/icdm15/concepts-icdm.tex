\documentclass[conference]{IEEEtran}
\usepackage{mathtools}
\usepackage{color}

\usepackage{listings}
\lstdefinelanguage{Scala}{}
\lstset{language=Scala, basicstyle={\small\ttfamily}}

% For commenting
%\newcommand{\comment}[1]{{\small \color{red} {#1}} \normalcolor}
% Use this line for leaving out comments
\newcommand{\comment}[1]{}

% Todo
%\newcommand{\todo}[1]{{\small \bf \color{red} {#1}} \normalcolor}
% Use this line to leave out
\newcommand{\todo}[1]{}

% Notation commands (change all by changing here)
% Defina a capital Rho
\newcommand{\Rho}{\mathrm{P}}
% Concept relation
\newcommand{\rn}[1]{\rho_{#1}}
% Concept L1 norm
\newcommand{\rns}[1]{|\rn{#1}|_1}
% Concept relation threshold
\newcommand{\mrn}[1]{\tau_{#1}}
% Discarded concept relation sum
\newcommand{\drns}[1]{|\check{\rho}_{#1}|_1}
% Kept concept relation sum
\newcommand{\krns}[1]{|\hat{\rho}_{#1}|_1}
% Concept relation vector
\newcommand{\rv}{\Rho}

% Concept similarity
\newcommand{\sy}[1]{\sigma_{#1}}
% Approximate concept similarity
\newcommand{\asy}[1]{\tilde{\sigma}_{#1}}
% L1 norm of difference
\newcommand{\nm}[1]{L_1(#1)}
\newcommand{\dnm}[2]{|\rn{#1}-\rn{#2}|_1}
% Approximate L_1 norm
\newcommand{\anm}[1]{\tilde{L}_1(#1)}

\newcommand{\anonymous}{\emph{Withheld to retain anonymity.}}

% Correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor kompetens-utveckling}

\begin{document}

%\title{Knowing an object by the company it keeps:\\Scalable concept discovery through commutability}
\title{Knowing an Object by the Company It Keeps:\\A Domain-Agnostic Scheme for Similarity Discovery}
%\title{Knowing Objects by the Companies They Keep:\\A General-Purpose Scheme for Similarity Discovery}

% Keep before submission to check page limit.
\author{
\IEEEauthorblockN{Olof G\"{o}rnerup}
\IEEEauthorblockA{Swedish Institute of\\
Computer Science (SICS)\\
SE-164 29 Kista, Sweden\\
Email: olof@sics.se}	
\and
\IEEEauthorblockN{Daniel Gillblad}
\IEEEauthorblockA{Swedish Institute of\\
Computer Science (SICS)\\
SE-164 29 Kista, Sweden\\
Email: dgi@sics.se}
\and
\IEEEauthorblockN{Theodore Vasiloudis}
\IEEEauthorblockA{Swedish Institute of\\
Computer Science (SICS)\\
SE-164 29 Kista, Sweden\\
Email: tvas@sics.se}
}

\maketitle

\begin{abstract}
Appropriately defining and then efficiently calculating similarities from large data sets are often essential
in data mining, both for building tractable representations and for gaining understanding of data and generating processes.
Here we rely on the premise that given a set of objects and their correlations, each object is characterized
by its context, i.e.\ its correlations to the other objects, and that the similarity between two objects therefore can be 
expressed in terms of the similarity between their respective contexts.
Resting on this principle, we propose a data-driven and highly scalable approach for discovering
similarities from large data sets by representing objects and their relations as a correlation graph that is
transformed to a similarity graph. Together these graphs can express rich structural properties
among objects. Specifically, we show that concepts -- representations of 
abstract ideas and notions -- are constituted by groups of similar objects that can be identified
by clustering the objects in the similarity graph.
These principles and methods are applicable in a wide range of domains, and will here be demonstrated 
for three distinct types of objects: codons, artists and words, where the numbers of objects and correlations
range from small to very large.
\end{abstract}

%Note: ICDM omits keywords.
%\IEEEpeerreviewmaketitle

\section{Introduction}
\label{sec:introduction}

As stated by Firth \cite{Firth57} and further popularized in the computational linguistics community by Church and
Hanks \cite{Church90}, ``You shall know a word by the company it keeps''. Departing from this principle, which can be 
traced further back to analytic philosophy, there have been substantial efforts to infer semantic and syntactic meaning
 from words through their effective usage in text \cite{Harispe2015}. Although the same principle has been applied in 
 different and seemingly distinct domains, such as bibliometrics  \cite{Kessler1963} and bioinformatics 
 \cite{ravasz2002hierarchical}, generalizing the notion of characterizing objects through 
their contexts into a broader fundamental principle for similarity discovery is so far largely unexplored.

Extending Firth's line of thought we argue that, with respect to observed data, the effective semantics of any object are given 
by the context in which it occurs, or in other words, by how it is related (or correlated) to all other objects. The \emph{similarity} 
between two objects may therefore be formulated in terms of their contexts, or how similar their relations to all other
objects are. %In essence, we focus on the commutability (or exchangeability) of objects.
A benefit of this is that we can omit the specific functionality or underlying workings of objects, but
only observe and consider their context patterns. This is highly attractive from a data-driven machine learning
perspective since it requires very few assumptions about the objects.

With this as a starting point, we propose a graph-based method for discovering similarities from large data
sets. An \emph{object} is intentionally left vague since it can be many different things, such as
music tracks in a playlist, people in a social network, tokens in a text or states in a stochastic process. We narrow down the scope
slightly by only considering objects that exhibit pairwise relations, e.g.\ in terms of spatial, temporal or social
correlations, which allows us to represent a collection of objects and their inter-dependencies as a graph. Our
approach, which we call \emph{Contextual Correlation Mining} (CCM), involves two main steps: First, we create a \emph{correlation graph} 
that describes the pairwise correlations between all objects. A correlation may here be any relationship measure such as the frequency 
of co-occurrence, a transition probability in a stochastic process, a correlation measure such as mutual information or a weighted edge in a graph. 
We then transform the correlation graph to a \emph{similarity graph} by comparing the set of correlations of each object
to the sets of correlations of all other objects -- the more similar sets of correlations, the higher the weighted edge in the
similarity graph.

The correlation graph is either given at the outset, as a Markov model or co-occurrence network for example, or built from data.
Since there already exists a multitude of approaches for achieving this, see e.g. \cite{Albert2002}, we will here focus on the second 
step, which we also view as the main technical contribution of this paper. Transforming a correlation graph to a similarity graph 
is conceptually straightforward, but as an ``all-to-all" similarity
problem, it is highly challenging in practice. However, since we are considering
pairwise correlations, we can utilize that similar objects always occur in proximity in the correlation graph (at most one neighbour apart to 
be specific), which means that it is sufficient to compare objects locally in the graph. This not only drastically reduces the number 
of necessary comparisons,
but also facilitates parallelization. Moreover, given that the correlation graph is sparse\footnote{That is, most objects are 
either completely unrelated or at most negligibly correlated. Two randomly selected persons in a large social network, for 
instance, most likely do not know each other.} -- which is the case e.g.\ for gene co-expression \cite{Jordan2004}, semantic
 \cite{Steyvers2005}, word co-occurrence \cite{Cancho2001} and social networks \cite{mislove2007social}, as well as for 
 many other graphs of interest \cite{Albert2002} -- we can also prune the correlation graph substantially 
prior to transforming it to a similarity graph while keeping the approximation error low and controllable.

In comparison, related methods are either limited to specific domains 
or do not scale well with growing number of objects, while the approach presented here is both highly scalable and agnostic
with respect to objects and correlation measures. These are merely seen as vertices and edges in a graph, and CCM
is therefore applicable in a broad range of domains as well as in mixed-data scenarios where several different correlation measures
may be considered. In this way, we propose a powerful and efficient scheme that distills the essence in many related, and seemingly 
distinct, methods by using the core principle that objects can be characterized by the contexts in which they occur.

Furthermore, since CCM does not require any intermediate representations of objects and their correlations, such as sparse vectors
 or neural networks, it is also interpretable and transparent. This enables us to calculate well-understood notions
of similarity and error among other things. Representing objects, correlations and similarities as graphs will also allow us
 to capture rich higher-scale structures among objects -- e.g.\ without being constrained by geometric properties such as the triangle 
inequality -- including ambiguity, concept hierarchies and ontologies, both in terms of correlations and similarities.
Rather than representing data in terms of its raw constituents, a central task then is often to discover appropriate levels of abstraction 
of objects, both for gaining insights about data and by computational necessity. As an illustrative example, it may 
for instance not be appropriate to analyze a large text corpus in terms of its individual characters, when the data can be described in 
terms of words or on more abstract levels still. We will here demonstrate that CCM can be used for this purpose. Specifically, we will show 
that \emph{concepts} -- coarse-grained abstractions of objects -- are constituted by groups of inter-similar objects that play 
analogous roles in data, and that we can discover these by clustering the objects in the similarity graph. 

\subsection{Outline}
The remainder of the paper is outlined as follows: Next we will put the paper in context by giving an overview of the related
state-of-the-art. A background with preliminaries is presented in Sec.\ \ref{sec:background}, followed by a description of the
proposed method in Sec.\ \ref{sec:methods}. In Sec.\ \ref{sec:experiments} we demonstrate the versatility of the method by
applying it in three distinct domains, with proof-of-concepts in computational linguistics, music and molecular biology. Sec.\
\ref{sec: scalability} treats the scaling properties of the method, where we show that it is scalable both in theory and
practice. The paper is concluded in Sec.\ \ref{sec:conclusions} with a summary of our findings and a discussion on possible
future directions.

\section{Related work}
\label{sec:related work}

The principle of relating objects with respect to contextual information is employed in several different areas, including ontology 
learning, computational linguistics, bioinformatics and bibliometrics. The method that is closest in spirit to 
ours is SimRank \cite{Jeh2002simrank}, which is a general approach for obtaining similarities between vertices in a graph.
 SimRank is an iterative method that uses the graph structure to derive similarities between objects by relating ``objects that are 
 related to similar objects'' \cite{Jeh2002simrank}. The main drawback with their approach, however, is that it is not scalable due to 
 a cubic time complexity with the number of vertices in the graph. This has partly been remedied in improved versions of the algorithm, 
 such as the one by Yu et. al \cite{Yu2012simrankOpt}, but these are still too computationally demanding in order to be applicable on
  very large graphs. In comparison, we can comfortably run our algorithm on graphs with tens of millions of edges, doing only a single 
  pass over the data. Ravasz et al.\ propose a related approach for finding similar vertices using so called 
  topological overlap measures \cite{ravasz2002hierarchical}, which they apply on metabolic networks. Zhang et al. \cite{Zhang2005tom}
  generalized this approach for use on weighted gene co-expression networks. As in our case, these methods relate vertices by assigning 
  higher similarity scores to vertices that share many neighbors, but since their approaches are primarily tailored for bioinformatics tasks, 
  they lack the generality of SimRank and the method presented here.
  
In computational linguistics, distributional analysis -- where linguistic items are characterized by
their relative distributional properties in the data -- has become a fundamental approach \cite{Harris-1970}. We use
similar assumptions as a starting point, and when applied to text, the approach can be seen as transforming a graph
over syntagmatic similarities to one describing paradigmatic similarities \cite{Sahlgren-2006}, in which
concepts are discovered through clustering. A large number of methods to find semantic similarities have been
developed -- see \cite{Harispe2015} for a recent review -- from the seminal work by Church and Hanks \cite{Church90}, and Brown 
et al.\ \cite{Brown1992}, to more recent approaches, e.g.\ based on vector representations, such as GloVe 
\cite{Pennington2014}, and neural networks, such as word2vec \cite{Mikolov-2013}. 
Several of these methods could be used to produce the equivalent of the similarity graph in which we perform
clustering to find concepts. These methods, however, are limited to natural language processing while our approach 
is domain-agnostic. Another important difference is that our method builds similarity graphs without 
using any dimensionality reduction or intermediate representations, such as high-dimensional vectors 
or difficult-to-interpret neural networks. The advantage of using a direct graph representation is that it allows us to
understand and reason about higher-scale structures among objects and concepts, such as hierarchical organization, in a straightforward 
manner using established graph and network methods. Although graph representations are used in natural language processing to relate 
similar words and documents \cite{Mihalcea2011}, these approaches have several limitations in comparison to our 
approach, e.g.\ by expecting existing similarity graphs as input, using ad hoc word relations (such as linking words separated 
by \emph{and} or \emph{or}), requiring part-of-speech tagged data, or by using human curated datasets, such as WordNet \cite{miller1995wordnet}.

Another related area is ontology learning \cite{Wong2012ontology}, which aims to infer taxonomies 
from corpora and other data sources. While one can draw parallels between our work and this field, the latter is often limited 
by exclusively considering a specific type of basic building blocks, such as nouns, where these are related in 
 hierarchies with respect to specific relations, such as \emph{is a} and \emph{part of}. Similarly, context-based similarity
  discovery can also be viewed as a generalization of methods in bibliometrics, where citation patterns among a set of 
  documents, such as scientific papers, are studied. Using so called bibliographic coupling to relate papers  \cite{Kessler1963} 
  -- i.e.\ the similarity between two papers is based on the number of  citations they share -- is a special case of our approach 
  for relating two objects in the correlation graph. Another resemblance is that  these and similar measures are used to cluster 
  scientific papers \cite{Small1973} as well as web pages \cite{Larson96}. The method presented here could be employed in 
  the very same way -- where binary correlations are given by citations -- to efficiently relate a large number of documents.

\section{Background}
\label{sec:background}

\subsection{Preliminaries}
\label{sec:preliminaries}

We begin by specifying the terminology used in this paper. Due to the transdisciplinary character of the method, we
 choose to use general rather than domain-specific terms.

Let $C = \{i\}_{i=1}^n$ be a set of \emph{objects}, where each object has a correlation, $\rn{i,j}$, to
each other object. This relation can be expressed in terms of real values, probabilities, booleans or something
else that, for instance, represent a correlation measure, binary or weighted neighbourhood relation in a graph,
co-occurrence probabilities in a corpus, or transition probabilities in a Markov chain. An object can for example be a 
word in text, and the correlations between words can be their co-occurrence probabilities. In another example, objects 
constitute people, and the correlation between two persons is their strength of friendship.

The \emph{context} of an object $i$ is considered to be its vector of relations to every other object, $\rn{i} =
(\rn{i,j})_{j=1}^n$. In our word example, the context of a word is therefore its correlations to all other words. Analogously, 
in the people example, the context of a person is all its friendships.

Under the assumption that an object is characterized by its context, we can formulate the
similarity between two objects $i$ and $j$, denoted $\sy{i,j}$, in terms of a similarity measure between their
respective contexts.
Here we define $\sy{i,j}$ to be 1 subtracted by the relative $L_1$-norm of the difference between $\rn{i}$ and $\rn{j}$:
%Here we define $\sy{i,j}$ to be the inverse of the relative $L_1$-norm of the difference between $\rn{i}$ and $\rn{j}$:
\begin{equation}\label{eq:sim}
\sy{i,j} = 1 - \frac{\dnm{i}{j}}{\rns{i} + \rns{j}},
%\sy{i,j} = \frac{\rns{i} + \rns{j}}{\dnm{i}{j}},
%\sy{i,j} = \frac{\rns{i} + \rns{j}}{\nm{i,j}},
\end{equation}
where
\begin{equation}\label{eq:totrel}
\rns{i} = \sum_{k \in C} | \rn{i,k}|
\end{equation}
and
\begin{equation}\label{}
\dnm{i}{j} =  \sum_{k \in C} | \rn{i,k} - \rn{j,k} |,
%\nm{i,j} =  \sum_{k \in C} | \rn{i,k} - \rn{j,k} |.
\end{equation}
denoted $\nm{i,j}$ for short.
%\comment{Consider using definition $\sy{i,j} = 1-\dnm{i}{j}/(\rns{i} + \rns{j})$ instead, such that $\sy{i,j} \in [0, 1]$ .}
That is, we normalize the absolute $L_1$-norm of the difference between $i$ and $j$:s context vectors with the maximum
possible norm of the difference, as given by $\rns{i} + \rns{j}$, and then subtract the result from one in order to
transform it to a similarity measure bounded by 0 and 1, $\sy{i,j} \in [0, 1]$.%invert the resulting relative norm in order to transform it to a similarity measure.

Since objects are discrete and have pairwise relations, we can represent $C$ and $\rn{i,j}$ as a directed graph,
$\mathcal{R} = (C, R)$, where vertices constitute objects, and where edges $r_{i,j} \in R$ have weights $\rn{i,j}$. We
term this the \emph{correlation graph} of $C$ with respect to $\rn{i,j}$. In principle this is a complete graph since
every vertex has a relation to every other vertex (including itself) through $\rn{i,j}$. However, we define the graph
such that there is only an edge between two vertices $i$ and $j$ if their corresponding objects have a degree of
similarity, i.e.\ when $\dnm{i}{j} < \rns{i} + \rns{j}$ and $i \neq j$. In our people-friendship example, the correlation network is simply a social network.

Analogously, the \emph{similarity graph} of $C$ with regard to $\rn{i,j}$, denoted $\mathcal{S} = (C, S)$, is defined
to be an undirected graph where weights of edges $s_{i,j} \in S$ instead are given by $\sy{i,j}$. %Note that $\mathcal{S}$ can also be a correlation graph (with respect to $\sy{i,j}$) that is associated with another, higher-order, similarity graph that in turn is associated with yet another similarity graph and so forth. \comment{This requires that we map $\mathcal{S}$ to a directed graph.}

By \emph{concept} we mean a group of objects that are approximately similar -- forming a cluster in 
the similarity graph -- and therefore approximately interchangeable in their respective contexts. In the word example 
this may correspond to a group of semantically and/or syntactically similar words (e.g.\ termed \emph{semantic community} 
or \emph{topic} in the natural language processing community), whereas in the social network example, a concept is a group of 
people that have similar circles of acquaintances.

\subsection{Example}

\begin{figure}
\begin{center}
\includegraphics[width=0.9\columnwidth]{figures/examplegraphs.pdf}
\end{center}
\caption{A correlation graph is transformed to a similarity graph in which clustering is performed.}
\label{fig:examplegraphs}
\end{figure}

As a simple stylized example, consider the set of objects $C = \{a,b,c,d,e,f,g\}$ with the symmetric, binary correlation 
graph shown to the left in Fig.\ \ref{fig:examplegraphs}. Transforming this correlation graph to the similarity graph shown in 
the same figure using Eq.\ \ref{eq:sim}, the pairwise similarities become positive when two objects have overlapping contexts. 
Each of the two clusters in the figure is identified as a concept.

Note that in the case of the binary relationship graph, the $L_1$-norm between two objects, $i$ and $j$, is given by the 
number of neighbours that they do not share:
\begin{equation}
\dnm{i}{j} = |n_i \cup n_j| - |n_i \cap n_j| = |n_i| + |n_j| - 2 |n_i \cap n_j|,
\label{eq:binarynorm}
\end{equation}
where $n_i$ and $n_j$ are the neighbourhoods of $i$ and $j$. Since the maximum possible norm of the difference is $|n_i| + |n_j|$, 
the similarity between $i$ and $j$ becomes
\begin{equation}
\sy{i,j} = 1 - \frac{|n_i| + |n_j| - 2 |n_i \cap n_j|}{|n_i| + |n_j|} = \frac{2 |n_i \cap n_j|}{|n_i| + |n_j|},
\label{eq:binaryrelnorm}
\end{equation}
which is known as the S{\o}rensen-Dice coefficient \cite{Dice45,Sorensen48}, that, in turn, is analogous to the commonly 
used Jaccard coefficient \cite{Jaccard1912} through a monotonic transformation.

\section{Methods}
\label{sec:methods}

%As outlined in Sec.\ \ref{sec:introduction}, our approach consists of three steps: 1) correlation graph building,
%2) correlation to similarity graph transformation, and 3) object clustering. Since 2) constitutes the main
%technical contribution of this paper, we will here focus on this step, and use simple -- even trivial -- and
%transparent methods in steps 1) and 3) to facilitate our understanding of the resulting correlation to similarity graph transformations.

\subsection{Similarity calculations}
\label{sec:similaritycalculations}

In order to efficiently and scalably transform a correlation graph into a similarity graph, we utilize two observations. 
Firstly, an object only has a degree of similarity to its second-order
neighbours (its neighbours' neighbours) in the
correlation graph $\mathcal{R}$. Let $n_i$ and $n_j$ be the neighbouring vertices of $i$ and $j$ respectively, and $\rn{i,
k} = 0$ if $k \not\in n_i$. Then
\begin{eqnarray}
\nm{i,j}  =
\sum_{k \in n_i}  |\rn{i, k}| -  \sum_{\mathclap{k \in n_i \cap n_j}}  |\rn{i, k}| \notag\\
+  \sum_{k \in n_j}  |\rn{j, k}| -  \sum_{\mathclap{k \in n_i \cap n_j}}  |\rn{j, k}|
+  \sum_{\mathclap{k \in n_i \cap n_j}} |\rn{i, k} - \rn{j, k}| \notag\\
=  \rns{i} + \rns{j} + \sum_{\mathclap{k \in n_i \cap n_j}} (|\rn{i, k} - \rn{j, k}| - |\rn{i, k}| - |\rn{j, k}|). \label{eq:l1terms}
\end{eqnarray}
When calculating Eq.\ \ref{eq:sim} it is therefore sufficient to compare differences between weights $\rn{i, k}$ and
$\rn{j, k}$ of edges from $i$ and $j$ to neighbours $k$ that $i$ and $j$ have in common, give that we have the weight
sums of outgoing edges of $i$ and $j$.
In practice, we generate a similarity graph by first summing weights of outgoing edges per vertex, and then building an
intermediate undirected two-hop multigraph of $\mathcal{S}$, where an edge $(i, j)$ that corresponds to a hop through
$k$ in $\mathcal{S}$ has weight $|\rn{i, k} - \rn{j, k}| - |\rn{i, k}| - |\rn{j, k}|$. The $L_1$-norm between $i$ and
$j$ is then calculated by summing the weights of all edges between $i$ and $j$ in the multigraph according to Eq.\
\ref{eq:l1terms}, and adding this to the edge weight sums of $i$ and $j$.

\subsubsection{Approximations}
\label{subsec:approximations}
Even though we only need to consider shared neighbours when calculating the similarities between objects, these
calculations still scale unfavorably as the sum of the square of in-degrees per vertex, since we consider all pairs of
incoming edges of vertex $k$ when generating two-hop edges. We therefore need to approximate the similarity measure by
reducing in-degrees. To be able to determine whether a certain object distance with regard to a distance measure $D$ is
relevant or not, typically we would like to ensure that the error $E_{D}(i,j)$ in any specific distance approximation
is less than a fixed level $\theta_D$,
\begin{equation}
E_{D}(i,j) \leq \theta_D
\end{equation}
and more specifically for the $L_1$-norm approximated by $\tilde{L}_1$,
\begin{equation}
E_1(i,j) = | \nm{i,j}  - \anm{i,j}  | \leq \theta_1.
\end{equation}
If we would like to remove terms by approximating by zero while keeping the total approximation error $E_D$ 
as small as possible, we should remove the smallest correlation terms $\rn{i,k}$ in Eq.\ \ref{eq:l1terms}. Put differently,
we discard the edges with the smallest weights in the correlation graph.

Let $\mrn{i}$ be a threshold value below which correlations of object $i$ are approximated by zero, and $\drns{i}$ the 
norm of discarded correlations:
\begin{equation} \label{}
\drns{i} = \sum_{ \rn{i,k} < \mrn{i}} |\rn{i,k}|.
\end{equation}
The upper bound of the error is then given by
\begin{equation} \label{eq:errbound}
E_1(i,j) \leq \drns{i} + \drns{j},
\end{equation}
where $E_1(i,j)=\drns{i} + \drns{j}$ when the edges of discarded relations of $i$ and $j$ do not share any destination
vertex $k$. When calculating the object similarity based on the $L_1$-norm, we can therefore reduce the number of
terms we need to compare by removing low correlation values with predictable errors. Lowering the number of terms in
Eq.\ \ref{eq:l1terms} while guaranteeing an error $E_1(i,j) \leq \theta_1$ is then a matter of sorting correlations
$\rn{i,k}$ and, starting with the smallest one, removing all relations until the cumulative sum exceeds half the
distance error threshold, $\theta_1/2$.

This brings us to our second observation, which is that in most correlation graphs of interest, a substantial fraction of
the correlations from one object to others are, if not zero, very small or even magnitudes smaller than its largest
relations, as exemplified in Fig.\ \ref{fig:billion-ew-cdf}. Thus, we may effectively prune a
large fraction of the links while keeping the cumulative discarded weight (and error) comparatively low, further reducing
computational complexity.

Moreover, if reducing terms in Eq.\ \ref{eq:l1terms} has priority over accuracy, we may start at the other end by specifying a 
maximum in-degree per vertex, and keep the corresponding number of incoming edges with the largest weights. Doing so 
we utilize that  the main bulk of vertices have low in-degrees and are therefore not affected by the
pruning. This situation is illustrated in Fig.\ \ref{fig:billion-id-cdf}. By calculating and storing
the sums of discarded weights of outgoing edges per vertex, we can then readily calculate the error bound per object
pair according to Eq.\ \ref{eq:errbound}.

\begin{figure}
\begin{centering}
\includegraphics[width=1.0\columnwidth]{figures/billion-ew-cdf.pdf}
\end{centering}
\caption{The cumulative distribution function of edge weights in the Billion word correlation graph described in 
Sec.\ \ref{subsec:words} shows that a large fraction of edges with low weights can be pruned. For example, 
approximately 90\% of the edges are discarded when considering edges with weights $\geq 0.01$.}
\label{fig:billion-ew-cdf}
\end{figure}

\begin{figure}
\begin{centering}
\includegraphics[width=1.0\columnwidth]{figures/billion-id-cdf.pdf}
\end{centering}
\caption{The cumulative distribution function of in-degrees for the graph referred to in Fig.\ \ref{fig:billion-ew-cdf}
 illustrates that it is possible to apply an in-degree threshold while affecting comparably few vertices. Only a small percentage
  of the vertices are affected, for instance, when capping the in-degree at 500 edges.}
\label{fig:billion-id-cdf}
\end{figure}

%If we instead use a mean correlation error limit $\theta_\xi$, this is as simple as removing all correlations $\rn{i,k} \leq \theta^\xi$. Note though that in any implementation where edges from $i$ are approximated as zero, it is important to calculate the error based on $\rn{i,k}$ and not $\rn{j,k}$ as this will lead to a tighter bound on the error (see equation \ref{eq:corrapproxbound}).

\subsection{Clustering}
\begin{sloppypar}
After transforming a correlation graph to a similarity graph, the latter typically exhibits tightly grouped objects that are
similar according to measure $\sy{i,j}$. We can therefore identify concepts by clustering the vertices, which is
also known as community detection. There is a large number of available algorithms with
varying suitability with regard to accuracy and scalability \cite{Fortunato2010}. However, it is beyond the scope of this paper to evaluate the
performance of different clustering algorithms in this context. Instead we use a simple and transparent clustering
method. The approach resembles standard distributed algorithms for identifying connected components in graphs and works as
follows: We begin by initializing each vertex $i$ to form its own cluster, indexed by $c_i = i$. Then, for each vertex $i$, we
set its cluster index to be the smallest cluster index of $i$:s neighbours $j$ for which $\sy{i,j} \geq \sigma_{min}$, where
$\sigma_{min}$ is a threshold value. This is repeated until no more cluster indices are changed. In this way, cluster memberships
are propagated within components that are separated by edges with weights $\sy{i,j} \leq  \sigma_{min}$. The interpretation of --
and rationale for -- this approach is that clusters in the graph are groups of vertices that are interlinked with a certain
degree of similarity, as specified by $\sigma_{min}$, and where the clusters, in turn, are interlinked with weaker similarity
relations.
\end{sloppypar}
%For the sake of transparency, 
%we employ a simple connected component-based approach for achieving this, but any of many available community detection 
%algorithms may be used for the same purpose \cite{Fortunato2010}. A possible future directions then is to iterate the procedure 
%in order to discover higher-order concepts, each constituted by concepts, that in turn form yet higher order concepts and so forth.

\subsection{Implementation}
\label{subsec:algorithmsAndImplementation}

\begin{figure*}
\begin{lstlisting}
1: ins   = edges.map(((i,j),rij) => (j,(i,rij)))
2: pairs = ins.join(ins).filter((k,((i,rik),(j,rjk))) => i<j)
3: terms = pairs.map((k,((i,rik),(j,rjk))) => ((i,j),abs(rik-rjk)-abs(rik)-abs(rjk)))
4:              .reducebykey((v,w) => v+w)
\end{lstlisting}
\caption{Pseudo-code of the sum term calculation in Eq.\ \ref{eq:l1terms}. 1) Edge tuples with vertex indices \texttt{i} and
 \texttt{j}, and weights \texttt{rij} are mapped to key-value pairs keyed by destination vertices. 2) A two-hop graph is 
 generated through self-join, and unique in-edge pairs are extracted through filtering. 3) All terms in the sum in 
 Eq.\ \ref{eq:l1terms} are calculated and 4) summed per two-hop neighbour pair.
}
\label{fig:pseudocode}
\end{figure*}

The calculations of the approximations and error bounds of the norms of the differences
$\dnm{i}{j}$, as formulated in Eq. \ref{eq:l1terms}, lend themselves well to functional
programming, since they can be implemented as a small number of standard transformations applied
on a collection of correlation graph edges. The procedure can be summarized in the following
steps:
\begin{enumerate}
\item Prune the correlation graph by filtering out edges with weights below a given threshold value, $\mrn{i}$, and/or 
by keeping a given number of incoming edges with the largest weights per vertex.
\item For each vertex $i$, calculate the norms $\rns{i}$ (the weight sum prior
to pruning), $\drns{i}$ (the weight sum of discarded edges) and $\krns{i}$
(the weight sum of kept edges), where $\drns{i}$ is simply acquired by
subtracting $\krns{i}$ from $\rns{i}$.
\item Calculate the sum term in Eq.\ \ref{eq:l1terms}, denoted $\Lambda_{i,j}$,
for each pair of vertices that share a neighbour in the pruned correlation graph.
This step is described in pseudo-code in Fig.\ \ref{fig:pseudocode} and
involves a self-join operation for building a two-hop multigraph that links
second-order neighbours, followed by a map transformation for calculating the
terms in the sum, which subsequently are summed up per vertex pair by a reduce
operation.
\item For each vertex pair $(i,j)$ in the previous step, calculate the
approximate relative $L_1$-norm, $\tilde{l}_{i,j}$, as $\tilde{l}_{i,j} =
(\Lambda_{i,j} + \psi_{i,j})/\psi_{i,j}$ and the upper error bound,
$\epsilon_{i,j}$, as $\epsilon_{i,j} = (\drns{i} + \drns{j})/\psi_{i,j}$,
where the normalizing factor $\psi_{i,j} = \rns{i} + \rns{j}$ is
the maximum possible difference between $i$ and $j$.
\end{enumerate}
After completing step 4 it is straightforward to calculate the approximate
similarity $\asy{i,j} = 1 - \tilde{l}_{i,j}$ according to Eq.\ \ref{eq:sim}.
Note that $\tilde{l}_{i,j}$ is a conservative approximation of the true
relative $L_1$-norm, $l_{i,j}$, since $\tilde{l}_{i,j} - \epsilon_{i,j} \leq
l_{i,j} \leq \tilde{l}_{i,j}$. For this reason, the acquired similarity
approximation will be the ``worst case scenario'' in the sense that it is always larger than the true relative $L_1$-norm.

The method is implemented in the Scala programming language and uses the in-memory
data processing framework Apache Spark \cite{Zaharia-2012}, which enables us
to employ the method at scale in terms of computing hardware.
To facilitate reproducibility, the implementation will be made available
with an open source license in an online repository.\footnote{https://github.com/sics-dna/concepts}
 Since we are exclusively using standard core primitives in Spark (\texttt{map},
\texttt{filter}, \texttt{join} etc.), implementing the
method in other similar frameworks, such as Apache Flink \cite{Alexandrov14}, is also
possible.

\section{Experiments}
\label{sec:experiments}

In order to demonstrate the broad applicability of our approach, we will now showcase it for three distinct 
types of objects: words, artists and codons. Here we prioritize breadth over depth, and more in-depth evaluations 
of the method's performance with respect to specific applications will be topics of future publications.

\subsection{Words}
\label{subsec:words}

\begin{figure*}
\begin{center}
\includegraphics[width=0.9\textwidth]{figures/billion-words-example.pdf}
\end{center}
\caption{Examples of concepts in a word similarity graph based on the Billion word corpus are constituted by clusters 
of similar words. For sake of clarity, edges with weights $\sy{i,j} \geq 0.15$ are shown.}
\label{fig:billion-words-example}
\end{figure*}

We begin by relating words in terms of their co-occurrence in text, where two words, $i$ and $j$, co-occur if they both
appear within a window of $n$ words. In the simplest case, for $n = 2$, words therefore co-occur if they are adjacent.
There exist many different word association measures, see \cite{Pecina08} for a large number of examples, such as
pointwise mutual information \cite{Church90} and normalized versions thereof \cite{Bouma09}. Here we simply measure the
association between $i$ and $j$ as the relative frequency of $j$ occurring in $i$:s context, or, in other words, as the
conditional probability that a randomly selected word in a window that contains $i$, will be the word $j$. That is, $\rn{i,j}
\approx c_{i,j}/{c_i}$, where $c_i$ and $c_{i,j}$ are the number of occurrences of $i$, and $i$ together with $j$,
respectively. Note that this measure is not symmetric and so $\rn{i,j} \neq \rn{j,i}$ may be true. There likely exists more
appropriate measures, such as the aforementioned pointwise mutual information, with regard to specific applications.
However, for the purpose of demonstrating our approach, we believe the conditional probability measure suffices.

The method is applied on two datasets: the Billion word \cite{Chelba13} and
the Google Books n-gram \cite{Michel10,Lin12} corpora. The former consists of
nearly one billion tokens and originates from crawled online news texts. From
these we count the number of occurrences of bigrams (pairs of adjacent words) with words consisting only
of alphabetic characters. This results in approximately 8 million unique
bigrams and a vocabulary with roughly 0.3 million words. From the bigram counts
we relate words by their ordered adjacency.

Despite the comparably modest size of this corpus and the narrow context window, the method
manages to discover groups of words that reflect both syntactic and semantic
concepts. Examples of such concepts are shown in Fig.\ \ref{fig:billion-words-example},
where we see that the clusters correspond e.g.\ to specific nouns,
 (\emph{tablet}, \emph{laptop}, \emph{notebook} etc.), adjectives
 (\emph{chic}, \emph{trendy}, \emph{fashionable} etc.), or adverbs
 (\emph{strongly}, \emph{intensely}, \emph{vigorously}, etc.). Note that antonyms, in addition to synonyms, 
 may occur in the same group (e.g. \emph{warmer} and \emph{colder}). This highlights that the notion of similarity (here
 corresponding to what is termed \emph{relatedness} in the NLP field)
 is very much dependent on the choice of correlation measure. The correlation measure may therefore be both
 application and domain-specific, whereas the definition of similarity, \emph{given} the correlation measure, is domain-agnostic. 
 Accordingly, antonyms are indeed similar by definition with respect to the correlation measure used in this example. However, 
 for other correlation measures, possibly supporting negative correlations, antonyms may occur in separate concepts.

The Google Books n-gram dataset, which consists of 361
billion tokens for the English language version of the dataset, is used both to evaluate the scalability of the
method, which will be discussed in Sec.\ \ref{sec: scalability}, and to quantify the quality of resulting similarity
relations. An n-gram can be defined as a contiguous sequence of $n$ words in a text.
To further challenge the method, we apply it on correlation graphs with respect to co-occurrence
windows of size 5. This results in a denser correlation graph, since a
word has more neighbors due to the larger co-occurrence window size. Nevertheless, the
key properties that we describe in Sec.\ \ref{subsec:approximations} still
apply and we can prune away a large number of edges with low weights.

A common approach to quantitatively evaluate the performance of word association methods is to use benchmarks
with word pairs that have been manually graded with respect to degree of association. Since these
benchmarks also contain unassociated words, it is not possible to do a direct comparison between our
method and other
approaches in terms of benchmark performance, since our method exclusively relates words that have a certain
degree of similarity (indeed, this is one of the reasons it is scalable). However, to give an indication of
the method's performance, we measure the Spearman rank correlation coefficient between benchmark
similarities and $\sy{i,j}$ for word pairs $(i, j)$ that \emph{do} exist in the similarity graph. For this purpose we
use the standard WS-353 test collection \cite{Finkelstein01}, which consists of 353 word pairs that have
been graded by human annotators. We build a similarity graph from co-occurrence windows of size 5, filter
out words that occur with a frequency less than $10^{-8}$ and edges $\rn{i,j} < 10^{-3}$, and
set the maximum in-degree to 200. In this graph, which is built in less than 10 minutes (cf. Fig.\ \ref{fig:google-e-runtime}),
60\% of the WS-353 word pairs are present, resulting in a Spearman rank correlation of 0.76. The current state of the
art (with respect to the whole dataset) is 0.81 \cite{Halawi12,Yih12}.
These figures represent the correlation with respect to the average annotator score. Note, however,
that there is low inter-annotator agreement in WS-353, where the mean performance
of individual annotators, with respect to the mean score of the remaining annotators, is in fact also 0.76 \cite{Hill14}.

\subsection{Artists}
In the next proof-of-concept we relate artists by using a dataset that represents the listening
habits of users of the \emph{Last.fm} music service.\footnote{http://www.last.fm/} This dataset, provided by Celma
\cite{Celma2010}, consists of approximately 19 million track plays of 992 users. For each user, we extract sequences of
played artists - there are roughly 177000 in total - and consider the context of an artist to be defined by the probability
distribution of subsequently played artists. Hence, we assume artists are related in a Markov chain, where each artist
constitutes a state, and where there is a directed edge from artist $i$ to artist $j$ weighted with the probability
that $j$ is played next, given that $i$ is currently playing. This probability is simply estimated as $\rn{i,j} \approx
c_{i,j}/c_i$, where $c_i$ and $c_{i,j}$ are the number of times $i$, and $i$ followed by $j$ occur in the data set,
respectively.

The in-degree distribution of the artist correlation graph resembles those of the word correlation graphs,
which again means that relatively few vertices are affected by in-degree pruning. Transforming the artist correlation
graph to a similarity graph also results in tightly grouped artists that can be clustered, where the resulting
clusters appear to represent musical genres as exemplified in Fig.\ \ref{fig:artists}. As such, the similarity graph could
 be used in a music recommendation system to relate similar artists 
through the listening habits of users, similar to a collaborative filtering system. We could then also provide an
 intuitive way to incorporate the popularity of artists via their play frequencies in order to mitigate the 
 effect of popularity bias in recommendations \cite{celma2008hits}.

%This makes the system differ from the co-occurrence based artist similarity measure presented in \cite{schedl2005web}, 
%for example, where the authors define artist-to-artist similarity based on the number of online search engine results, as was the 
%approach in \cite{Cilibrasi-2007}. 

\begin{figure}
\begin{centering}
\includegraphics[width=0.95\columnwidth]{figures/last-fm-example-3.pdf}
\end{centering}
\caption{Examples of clusters in an artist similarity graph correspond to three distinct music genres. Edges with weights $\sy{i,j} \geq 0.5$ are shown.}
\label{fig:artists}
\end{figure}

\subsection{Codons}

Finally, we apply the method in molecular biology, where we consider codons as objects. Codons are triplets of
adjacent nucleotides in DNA that translate to amino acid residues that in turn form proteins.  These are related
through codon substitution dynamics, which is central both for understanding molecular evolution and in applications
such as DNA sequence alignment \cite{Anisimova09}. Since there are only 64 codons in total, this example differs from
the previous two in that we consider relatively few objects.

Codon substitutions are often modeled as Markov processes \cite{Anisimova09}, where the
substitution probabilities of a codon at a specific location are assumed to be independent of neighbouring codons as
well as previous codons at the same location. In this example we use an empirically derived codon substitution matrix 
provided by Schneider et al. \cite{Schneider2005}, where we consider the context of a codon $i$ to be given by the relative substitution frequencies
$(\rn{i,j})_{j=1}^n$ to other codons $j$.

\begin{figure}
\begin{centering}
\includegraphics[width=1.0\columnwidth]{figures/codon-example.pdf}
\end{centering}
\caption{Codon similarity graph where vertices are labeled with $c/a$ for codon $c$ coding to amino acid $a$. Edges
with weights $\sy{i,j} \geq 0.45$ are shown. Vertices are color coded with respect to amino acids and grouped by
properties. Note that when the edge weight threshold is lowered, clusters containing several amino acids are split by
amino acid. The rare and low mutable amino acid tryptophan is omitted. }
\label{fig:codons}
\end{figure}

As seen in the resulting codon similarity graph in Fig.\ \ref{fig:codons}, codons that translate to the same amino acid according
to the standard genetic code \cite{Nirenberg65} tend to be grouped. This reflects that codons that are highly similar
are commutable -- quite literary -- since substitutions between these codons are neutral under evolution. These clusters are also
present in the correlation graph and therefore preserved through the similarity graph transformation.

We now shift perspective and view ``amino acid'' as a concept. Again looking at Fig.\ \ref{fig:codons},
we see that some of the amino acids are grouped. This can be explained by a higher degree of neutrality within groups
than between them, which has been observed in empirical amino acid substitution matrices, such as the accepted point
mutation (PAM) matrix by Dayhoff et al. \cite{Dayhoff78}. In comparison, Wu and Brutlag derived amino acid substitution
groups by group-wise (as opposed to pairwise) statistical analysis of protein databases \cite{Wu96}.  The groups shown
in Fig.\ \ref{fig:codons} (\{I, L, M, V\}, \{K, R\} and \{N, S\}) all agree with their findings. In summary, the codon similarity 
graph captures both concepts and higher-order concepts: from codons to amino acids, via the genetic code,
to higher-order amino acids that constitute known substitution groups.

\section{Scalability}
\label{sec: scalability}

In order to enable practical use on large tasks in terms of the number of objects, correlations and example data,
a key design goal is scalability. Since we are using
relational primitives to represent graphs,
the scalability of the algorithm can be studied using established results from
relational algebra \cite{Chandra77, bitton83}. 

\begin{figure}
\begin{centering}
\includegraphics[width=1.0\columnwidth]{figures/billion-e-rt-2.pdf}
\end{centering}
\caption{Runtime, mean error bound and standard deviation of error bound (shown as error bars) for different in-degree 
thresholds, and $\rn{i,j} \geq 10^{-5}$. Built from bigrams in the Billion word corpus using a commodity laptop.}
\label{fig:billion-e-runtime}
\end{figure}

The most computationally demanding component of the algorithm is
building the two-hop graph through a self-join operation (the third step in Sec.\ \ref{subsec:algorithmsAndImplementation}).
Since a self-join is a conjunctive query \cite{Chandra77} in relational algebra terms,
we can reason about its computational cost.
Specifically for a distributed environment, Koutris et al. \cite{koutris2011conjuctive}
define a parallel algorithm as a sequence of parallel computation steps, and define its
cost as the number of steps required to complete the algorithm.
The authors prove that a join operation can be completed in one parallel computational
step using the hash-join algorithm, by using a communication and a computation phase.
Just as importantly they prove that the hash-join operation is load balanced
and as such it ensures linear speedup (doubling the server count reduces the load by
half) and constant scale-up (when doubling both the size of the data and number of servers, the running time remains the same).
Specifically for the Apache Spark platform, on which we implement the algorithm,
the self-join operation creates what Zaharia et al.\ \cite{Zaharia-2012}
call a \textit{narrow dependency}. This property allows for pipelined executions
of all operations on one node up until the reduction step in Fig. \ref{fig:pseudocode}, without the need for expensive data shuffles
through the network.

To demonstrate that our approach is applicable at scale in practice, we apply it on one of the largest, to our knowledge,
text corpora currently available, the Google Books
n-gram dataset \cite{Michel10,Lin12}, which corresponds to approximately 4\% of all books ever printed.
The dataset is publicly available, and in our experiments we use
the version that is available through the Amazon S3 service.\footnote{https://aws.amazon.com/datasets/8172056142375670}
As described in Sec.\ \ref{subsec:words}, we use the English language corpus
which contains approximately 361 billion tokens. When processed into 5-grams, the corpus results in a file with 24.5 billion 
rows and the total compressed size of the dataset is 221.5 GB. This data is pre-processed to create the correlation graph by retaining
only alphabetic characters. The resulting correlation graph before pruning has 706,108 vertices and 94,945,991 edges.

To perform the experiments we employ an Apache Spark cluster created using the Amazon
Web Services EC2 service.\footnote{http://aws.amazon.com/ec2/}
The cluster consists of 8 nodes (1 master and 7 slaves), where each node has 4 vCPUs and
30.5 GiB of memory (EC2 instance type \emph{r3.xlarge}), such that the total amount of memory available to the cluster
is roughly 186 GiB, as reported by Spark.

\begin{figure}
\begin{centering}
%\vspace{0.1cm}
\includegraphics[width=1.0\columnwidth]{figures/eng-all-edge-low-e3-vtx-low-e8-high-e1-100-400.pdf}
\end{centering}
\caption{Runtime, mean error bound and standard deviation of error bound (shown as error bars) for different in-degree 
thresholds, and $\rn{i,j} \geq 10^{-3}$. Built from Google Books 5-grams using an Amazon
EC2 cluster (see text for details).}
\label{fig:google-e-runtime}
\end{figure}

The experiment results support the theoretical investigation of the computational
cost of the algorithm, and together with the pruning
described in Section \ref{subsec:approximations} we are able to transform correlation graphs into similarity
graphs in reasonable amounts of time. This also holds true when using more modest computational resources,
as shown in Fig.\ \ref{fig:billion-e-runtime}, for building similarity graphs using the Billion word corpus as described
in Sec.\ \ref{subsec:words}. Analogous results are achieved in the Google 5-gram case, here with runtimes on the order of minutes,
as seen in Fig.\ \ref{fig:google-e-runtime}. The experiments were replicated three times, and
the runtimes are reported in Table \ref{tab:google_runtimes}.
Fig.\ \ref{fig:billion-e-runtime} and Fig.\ \ref{fig:google-e-runtime} also illustrate the trade-off between accuracy, controlled
via the in-degree threshold, and runtime, where the runtime scales favourably with an increasing in-degree threshold.
With respect to the in-degree threshold, we also observe a sublinear scaling of the number of edges in the correlation
graph, and a linear growth of the number of  edges in the similarity graph,  as shown in Fig.\ \ref{fig:google-ne}. This reflects the
situation exemplified in Fig.\ \ref{fig:billion-id-cdf}, namely that comparably few vertices are affected by the in-degree threshold.

\begin{figure}
\begin{centering}
\includegraphics[width=0.95\columnwidth]{figures/eng-all-edge-low-e3-vtx-low-e8-high-e1-100-400-idg-ne-nt.pdf}
\end{centering}
\caption{Number of edges in the correlation- and similarity graph, respectively, for different in-degree thresholds. Same 
configuration as in Fig.\ \ref{fig:google-e-runtime}.}
\label{fig:google-ne}
\end{figure}

\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\caption{Runtimes in seconds for Google Books dataset.}
\label{tab:google_runtimes}
\begin{center}
\begin{tabular}{c|r|r|r||c|c}
  \hline \bfseries In-degree  & \bfseries Run 1
  & \bfseries Run 2
  & \bfseries Run 3
  & \bfseries $\mu$
  & \bfseries $\sigma$
  \\
  \hline\hline 100
  & 246.7
  & 229.5
  & 236.7
  & 237.6
  & 8.6
  \\
  \hline 200
  & 603.7
  & 573.2
  & 575.4
  & 584.1
  & 16.9
  \\
  \hline 300
  & 1062.4
  & 998.3
  & 1031.2
  & 1030.7
  & 32.0
  \\
  \hline 400
  & 1535.5
  & 1602.5
  & 1554.0
  & 1564.0
  & 34.6
%  \\
%  \hline \emph{Sum} & 3448.5
%  & 3403.7
%  & 3397.5
%  & 3416.5
%  & 27.7
  \\
  \hline
\end{tabular}
\end{center}
\end{table}

%\begin{table}[!t]
%\renewcommand{\arraystretch}{1.3}
%\caption{A Simple Example Table}
%\label{table_example}
%\centering
%\begin{tabular}{c||c}
%\hline
%\bfseries First & \bfseries Next\\
%\hline\hline
%1.0 & 2.0\\
%\hline
%\end{tabular}
%\end{table}

\section{Conclusions}
\label{sec:conclusions}

This paper proposes a conceptually simple method for discovering similarities and concepts through transforming a
correlation graph to a similarity graph on which clustering is performed. As the method does not rely on any
intermediate representation or dimensionality reduction, it is applicable with few restrictions
to any domain in which a correlation graph can be constructed. Our experiments show that the approach not only can
detect similarities and concepts in several types of data, but also that it is computationally feasible for large-scale
applications with very large numbers of objects.

Due to the generality of the approach there is a vast number of possible directions to take. For instance, CCM can 
potentially be used to discover analogous objects in gene regulatory data or protein interaction networks, to provide 
recommendations from user data, or in general for detecting higher-order dynamics in 
discrete-valued stochastic processes. It then remains to quantitatively evaluate the properties of the scheme, for example in 
terms of application specific benchmark performance, approximation error and runtime. 

The main methodological challenge for future work revolves around how to efficiently build hierarchical concept models.
The concepts discovered through the methods described in this paper essentially represent OR-relations: All constituent
objects of a cluster are commutable, and the concept can be said to be observed if any of its constituents
are. Analogously, strong clusters detected in the correlation graph could be considered to represent AND-relations,
where the corresponding concept is observed when all of its constituents are. Both these types of
concepts can be identified, brought back into the estimation of the correlation graph, and the process
iterated, allowing for the discovery of complex higher-order relations. How to reliably and efficiently perform this
remains an area of further study.

\section*{Acknowledgment}
This work was funded by the Swedish Foundation for Strategic Research (\emph{Stiftelsen f\"or strategisk forskning}) 
and the Knowledge Foundation (\emph{Stiftelsen f\"or kunskaps- och kompetensutveckling}). 
The authors would like to thank the anonymous reviewers for their valuable comments.

% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
%\begin{thebibliography}{1}
%\bibitem{IEEEhowto:kopka}
%H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.
%\end{thebibliography}

%\newpage

\bibliographystyle{IEEEtran}
\bibliography{concepts-icdm}

\end{document}


